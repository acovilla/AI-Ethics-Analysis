import pandas as pd
import numpy as np
import re
from textblob import TextBlob

data = pd.read_csv('ChatGPT.csv', low_memory=False)

# Checking for null values in the Tweet column
print("Number of null values in Tweet column before filling: ", data['Tweet'].isnull().sum())

# Filling null values with empty strings
data['Tweet'] = data['Tweet'].fillna('')

# Checking again for null values in the Tweet column after filling
print("Number of null values in Tweet column after filling: ", data['Tweet'].isnull().sum())


# Custom function to preprocess
def dataPrep(text):
    if not isinstance(text, str):  # checking text is a string
        return "" 
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # to remove URLs
    text = re.sub(r'\@\w+|\#', '', text) # to remove mentions and hashtags
    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # to remove special characters
    text = text.lower() # converting text to lowercase
    return text

# Applying the dataPrep function to clean the text
data['CleanedTweet'] = data['Tweet'].apply(dataPrep)

# Function to apply Sentiment Analysis
def sentimentAnalysis(tweet):
    analysis = TextBlob(tweet)
    return analysis.sentiment.polarity

data['Sentiment'] = data['CleanedTweet'].apply(sentimentAnalysis)

# Summary statistics
print(data['Sentiment'].describe())

# Visualisation of the sentiment distribution
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style='darkgrid')
sns.histplot(data=data, x='Sentiment', bins=20, kde=True)
plt.title('Sentiment Distribution of #ChatGPT Tweets')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

# Topic Analysis

import gensim
from gensim import corpora
from gensim.models import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import nltk
nltk.download('stopwords')

nltk.download('punkt')

# Tokenization of the cleaned tweets
data['TokenizedTweet'] = data['CleanedTweet'].apply(lambda x: word_tokenize(x))

# Removing stopwords and short tokens 
stop_words = set(stopwords.words('english'))
data['TokenizedTweet'] = data['TokenizedTweet'].apply(lambda x: [token for token in x if token not in stop_words and len(token) > 1])

# Dictionary from the tokenized tweets
dictionary = corpora.Dictionary(data['TokenizedTweet'])

# Corpus from the tokenized tweets
corpus = [dictionary.doc2bow(tweet) for tweet in data['TokenizedTweet']]

# LDA model training with 5 topics and displaying of topics
num_topics = 5
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=2, random_state=42, eval_every=1)


topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

ethics_keywords = ["bias", "transparency", "accountability", "privacy", "discrimination", "responsibility", "job"]

def contains_ethics_keywords(tweet):
    return any(keyword in tweet for keyword in ethics_keywords)

data["EthicsRelated"] = data["CleanedTweet"].apply(contains_ethics_keywords)
ethics_data = data[data["EthicsRelated"]].copy()
# Summary statistics
print(data["EthicsRelated"].describe())
# Perform sentiment analysis on ethics_data
ethics_data['Sentiment'] = ethics_data['CleanedTweet'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Visualise the sentiment distribution
sns.histplot(data=ethics_data, x='Sentiment', bins=20, kde=True)
plt.title("Sentiment Distribution for Ethics-related Tweets")
plt.xlabel("Sentiment Polarity")
plt.ylabel("Frequency")
plt.show()

# Summary statistics
print(ethics_data['Sentiment'].describe())
ethics_keywords = ["ethics", "bias", "fairness", "privacy", "transparency", "accountability", "safety", "security", "trust", "responsibility", "regulation"]

ethics_data = data[data['CleanedTweet'].str.contains('|'.join(ethics_keywords), case=False)]


ethics_data = ethics_data.copy()

ethics_data['TokenizedTweet'] = ethics_data['CleanedTweet'].apply(lambda x: word_tokenize(x))

stop_words = set(stopwords.words('english'))
ethics_data['TokenizedTweet'] = ethics_data['TokenizedTweet'].apply(lambda x: [word for word in x if word not in stop_words and len(word) > 2])

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
import nltk
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

ethics_data['TokenizedTweet'] = ethics_data['TokenizedTweet'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
dictionary = corpora.Dictionary(ethics_data['TokenizedTweet'])
corpus = [dictionary.doc2bow(text) for text in ethics_data['TokenizedTweet']]

num_topics = 5
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=3, random_state=42)

topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

lda_model2 = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)

topics = lda_model2.print_topics(num_words=5)
for topic in topics:
    print(topic)
ethics_data['lda_doc_topics'] = [lda_model2[doc] for doc in corpus]
# Assign topic labels to each tweet
def get_topic_label(topic_dist):
    return max(topic_dist, key=lambda x: x[1])[0]

ethics_data['TopicLabel'] = ethics_data['lda_doc_topics'].apply(get_topic_label)

# Filter the dataset to only include tweets with the topic labels corresponding to topics 3 and 4
focused_topics_data = ethics_data[(ethics_data['TopicLabel'] == 3) | (ethics_data['TopicLabel'] == 4)]
# Word count from focused topics
from collections import Counter

word_count = Counter()
for tweet in focused_topics_data['TokenizedTweet']:
    word_count.update(tweet)

print(word_count.most_common(10))
focused_topics_data = ethics_data[(ethics_data['TopicLabel'] == 3) | (ethics_data['TopicLabel'] == 4)].copy()

nltk.download('vader_lexicon')
# Sentiment from focused topics
from nltk.sentiment import SentimentIntensityAnalyzer

sia = SentimentIntensityAnalyzer()

def sentiment_score(tweet):
    text = ' '.join(tweet)
    return sia.polarity_scores(text)['compound']

focused_topics_data['SentimentScore'] = focused_topics_data['TokenizedTweet'].apply(sentiment_score)

print(focused_topics_data['SentimentScore'])

# Summary statistics
print(focused_topics_data['SentimentScore'].describe())
plt.hist(focused_topics_data['SentimentScore'], bins=20, edgecolor='black')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.title('Histogram of Sentiment Scores')
plt.show()
# Relationship between sentiment and the identified ethical topics.
grouped_by_topic = focused_topics_data.groupby('TopicLabel')

average_sentiment_by_topic = grouped_by_topic['SentimentScore'].mean()

# Summary statistics
print(grouped_by_topic['SentimentScore'].describe())

average_sentiment_by_topic.plot(kind='bar')
plt.xlabel('Topic Label')
plt.ylabel('Average Sentiment Score')
plt.title('Average Sentiment Score by Ethical Topic')
plt.show()

correlation_matrix = focused_topics_data[['TopicLabel', 'SentimentScore']].corr()


sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation Matrix")
plt.show()

# Demographic and geographic distribution of opinions regarding AI ethics.
!pip install geopy
!pip install geotext

import re
import pandas as pd
from geopy.geocoders import Nominatim
from geotext import GeoText



# Clean location data using geopy
def clean_location(location):
    geolocator = Nominatim(user_agent="ai_ethics_analysis")
    try:
        loc = geolocator.geocode(location, timeout=10)
        if loc:
            return loc.address
    except:
        return None
    return None


# Extractng countries from cleaned locations using geotext
def extract_country(location):
    if location:
        country = GeoText(location).country_mentions
        if country:
            return list(country.keys())[0]
    return None


# Cleaning and preprocessing of location data
focused_topics_data['CleanedLocation'] = focused_topics_data['Location'].apply(clean_location)
focused_topics_data['Country'] = focused_topics_data['CleanedLocation'].apply(extract_country)


# Extracting gender from user descriptions 
def extract_gender(description):
    male_keywords = ["male", "man", "boy"]
    female_keywords = ["female", "woman", "girl"]

    if not isinstance(description, str):
        return "unknown"

    if any(keyword in description.lower() for keyword in male_keywords):
        return "male"
    elif any(keyword in description.lower() for keyword in female_keywords):
        return "female"
    else:
        return "unknown"




focused_topics_data['Gender'] = focused_topics_data['UserDescription'].apply(extract_gender)
# Grouping the dataset by demographic categories
gender_sentiment = focused_topics_data.groupby('Gender')['SentimentScore'].mean()

# Grouping the dataset by geographic location
country_sentiment = focused_topics_data.groupby('Country')['SentimentScore'].mean()

# Summary statistics
print(gender_sentiment)

# Summary statistics
print(country_sentiment)

# Visualise sentiment scores across demographic groups
gender_sentiment.plot(kind='bar')
plt.title('Sentiment Scores by Gender')
plt.xlabel('Gender')
plt.ylabel('Average Sentiment Score')
plt.show()

# Visualise sentiment scores across countries
country_sentiment.plot(kind='bar', figsize=(10, 5))
plt.title('Sentiment Scores by Country')
plt.xlabel('Country')
plt.ylabel('Average Sentiment Score')
plt.show()

# Descriptive analysis.
topic_counts = focused_topics_data['TopicLabel'].value_counts()

print(topic_counts)
tweets_by_country = focused_topics_data['Country'].value_counts()

print(tweets_by_country)
# Visualise 
tweets_by_country.plot(kind='bar')
plt.title('Tweets per Country')
plt.xlabel('Country')
plt.ylabel('Number of Tweets')
plt.show()
correlation_influence_sentiment = focused_topics_data[['UserFollowers', 'SentimentScore']].corr()

print(correlation_influence_sentiment)
# Heatmap
sns.heatmap(correlation_influence_sentiment, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation Matrix")
plt.show()
average_sentiment_by_gender = focused_topics_data.groupby('Gender')['SentimentScore'].mean()
topic_counts_by_gender = focused_topics_data.groupby('Gender')['TopicLabel'].value_counts()

print(average_sentiment_by_gender)
print(topic_counts_by_gender)

#Improving visualisations for better understanding

import matplotlib.pyplot as plt

word_counts = [('chatgpt', 8509), ('bias', 1611), ('ethic', 888), ('cybersecurity', 878), 
               ('trust', 859), ('data', 789), ('openai', 768), 
               ('biased', 720), ('amp', 715), ('security', 605)]

# Unzip the list of tuples into two lists
words, counts = zip(*word_counts)

# Create a bar plot
plt.figure(figsize=(10, 8))  # Optional: You can set the figure size
plt.barh(words, counts, color='skyblue')  # Plot horizontal bar plot
plt.xlabel('Count')  # x-axis label
plt.ylabel('Words')  # y-axis label
plt.title('Word Counts')  # Plot title
plt.show()  # Display the plot

import seaborn as sns

df_sentiment = pd.DataFrame({
    'SentimentScore': [0.7650, 0.3612, 0.9735, -0.1779, 0.4404, 0.7650, -0.1027, 0.0000, 0.8316, 0.8957]
})

# Create a new column for sentiment category
df_sentiment['SentimentCategory'] = pd.cut(
    df_sentiment['SentimentScore'], 
    bins=[df_sentiment['SentimentScore'].min(), 0, df_sentiment['SentimentScore'].max()], 
    labels=['negative', 'positive'],
    include_lowest=True
)

# Convert SentimentCategory to object type
df_sentiment['SentimentCategory'] = df_sentiment['SentimentCategory'].astype('object')

# Handle neutral (0.0) sentiment separately
df_sentiment.loc[df_sentiment['SentimentScore'] == 0, 'SentimentCategory'] = 'neutral'

# Plot histogram with different colors for each sentiment category
plt.figure(figsize=(10, 6))
sns.histplot(df_sentiment, x='SentimentScore', hue='SentimentCategory', palette='coolwarm')
plt.title('Sentiment Score Distribution')
plt.show()  

import pandas as pd
import geopandas as gpd
import geoplot as gplt

# Assuming this is your sentiment data
sentiment_data = pd.Series({
    'AR': 0.151825, 'AU': 0.540250, 'BE': 0.457700, 'BR': 0.719650, 'CA': 0.217750,
    'CH': 0.175725, 'CO': -0.254857, 'DE': 0.347992, 'ES': 0.251400, 'FI': -0.148000,
    'FR': 0.018565, 'GB': 0.119500, 'GH': 0.440400, 'HK': -0.226300, 'IN': 0.064405,
    'IT': 0.000000, 'JM': 0.624900, 'KE': 0.313678, 'LT': 0.125000, 'MD': 0.599400,
    'MX': 0.718400, 'MY': -0.163100, 'NG': 0.726900, 'NL': 0.510600, 'NZ': 0.510600,
    'PA': 0.000000, 'PH': -0.273200, 'PK': 0.123100, 'PT': 0.790600, 'SE': 0.460150,
    'SG': 0.164550, 'SK': 0.735100, 'US': 0.218396, 'VE': 0.557400, 'ZA': 0.253214,
}, name='SentimentScore')

# Load geopandas built-in world map
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# Make sure the country codes in world map match with the codes in sentiment_data
world['iso_a2'] = world['iso_a2'].replace('UK', 'GB')

# Merge world map and sentiment_data
world_sentiment = pd.merge(world, sentiment_data.reset_index().rename(columns={'index': 'iso_a2'}), how='left', on='iso_a2')

# Plot world heatmap
fig, ax = plt.subplots(1, 1, figsize=(15, 10))
world_sentiment.boundary.plot(ax=ax, linewidth=1)
world_sentiment.plot(column='SentimentScore', ax=ax, legend=True, cmap='coolwarm')
plt.title('World Sentiment Heatmap')
plt.show()

import pandas as pd
import plotly.graph_objects as go

# Create a dataframe with sentiment scores
sentiment_data = {
    "Country": ["ARG", "AUS", "BEL", "BRA", "CAN", "CHE", "COL", "DEU", "ESP", "FIN", "FRA", "GBR", "GHA", "HKG", "IND", "ITA", "JAM", "KEN", "LTU", "MDA", "MEX", "MYS", "NGA", "NLD", "NZL", "PAN", "PHL", "PAK", "PRT", "SWE", "SGP", "SVK", "USA", "VEN", "ZAF"],
    "SentimentScore": [0.151825, 0.540250, 0.457700, 0.719650, 0.217750, 0.175725, -0.254857, 0.347992, 0.251400, -0.148000, 0.018565, 0.119500, 0.440400, -0.226300, 0.064405, 0.000000, 0.624900, 0.313678, 0.125000, 0.599400, 0.718400, -0.163100, 0.726900, 0.510600, 0.510600, 0.000000, -0.273200, 0.123100, 0.790600, 0.460150, 0.164550, 0.735100, 0.218396, 0.557400, 0.253214]
}

df = pd.DataFrame(sentiment_data)

# Create the plot
fig = go.Figure(data=go.Choropleth(
    locations = df['Country'], # Spatial coordinates (ISO Alpha-3 country codes)
    z = df['SentimentScore'], # Data to be color-coded
    text = df['Country'], # Country names to display on hover
    colorscale = 'RdYlGn', # Choose colorscale
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_title = 'Sentiment Score',
))

# Update layout
fig.update_layout(
    title_text='World Sentiment Score',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()


# Create a dataframe with sentiment scores
sentiment_data = {
    "Country": ["ARG", "AUS", "BEL", "BRA", "CAN", "COL", "DEU", "ECU", "EST", "ESP", "FIN", "FRA", "GBR", "GEO", "HKG", "IDN", "IND", "ITA", "CYM", "USA", "VEN", "ZAF"],
    "SentimentScore": [0.657450, 0.340388, 0.644500, 0.059300, 0.318084, 0.175060, 0.169500, 0.731500, -0.812600, 0.016725, -0.624900, 0.126857, 0.253168, -0.778300, -0.156133, 0.000000, 0.098664, 0.593633, -0.077200, 0.253230, 0.179533, 0.366682]
}

df = pd.DataFrame(sentiment_data)

# Create the plot
fig = go.Figure(data=go.Choropleth(
    locations = df['Country'], # Spatial coordinates (ISO Alpha-3 country codes)
    z = df['SentimentScore'], # Data to be color-coded
    text = df['Country'], # Country names to display on hover
    colorscale = 'RdYlGn', # Choose colorscale
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_title = 'Sentiment Score',
))

# Update layout
fig.update_layout(
    title_text='World Sentiment Score',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()

# Create a dataframe with tweets per country
tweets_data = {
    "Country": ["USA", "GBR", "IND", "CAN", "FRA", "ZAF", "DEU", "AUS", "HKG", "PHL", "COL", "NZL", "ESP", "PAK", "BRA", "VEN", "ITA", "SWE", "SGP", "NOR", "PRT", "NLD", "UGA", "ARG", "BEL", "CYP", "IDN", "FIN"],
    "Tweets": [229, 56, 44, 32, 14, 11, 8, 8, 6, 6, 5, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1]
}

df = pd.DataFrame(tweets_data)

# Create the plot
fig = go.Figure(data=go.Choropleth(
    locations = df['Country'], # Spatial coordinates (ISO Alpha-3 country codes)
    z = df['Tweets'], # Data to be color-coded
    text = df['Country'], # Country names to display on hover
    colorscale = 'Reds', # Choose colorscale
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_title = 'Number of Tweets',
))

# Update layout
fig.update_layout(
    title_text='World Tweets per Country',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()  
